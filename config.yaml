# Speech module defaults: audio capture, STT, TTS, and single prompt for STT -> one sentence.
# Merged with root config.yaml; root overrides these.

# Single prompt for the speech flow (regeneration + main completion). Configurable here or via root modules.speech.prompt.
prompt:
  system: |
    You complete a speech-impaired user's partial utterance into exactly one natural sentence they meant to say. The input is often fragmented or misheard (e.g. "hockey" for "I'm"). Output only that one sentence as the user would say it to a caregiverâ€”first person for statements ("I want water.", "My leg hurts."), direct requests ("Pass me the salt."), or the question they are asking ("Do you have the time?"). No explanation, no preamble, no description of the task. If the input is already a clear, complete sentence (e.g. "Test sentence.", "I want water.", "Hello."), output that same sentence with high certainty. Only if the input is truly unintelligible noise or gibberish, output exactly: I didn't catch that. Never use "I didn't catch that" for test phrases, greetings, or clear words.
  user_template: "Complete this phrase into one sentence the user meant to say: {transcription}"

audio:
  device_id: null
  sample_rate: 16000
  chunk_duration_sec: 5
  sensitivity: 3.0
  auto_sensitivity: false
  auto_sensitivity_min_level: 0.002
  auto_sensitivity_max_level: 0.08
  auto_sensitivity_step: 0.25
  auto_sensitivity_cooldown_chunks: 3

stt:
  engine: whisper   # use "vosk" for much faster, lower-latency transcription
  vosk:
    model_path: "models/vosk-model-small-en-us-0.15"
  whisper:
    model_path: "base"   # base=fastest, small/medium/large=slower, more accurate
    device: "auto"       # auto | cpu | cuda (auto uses GPU if available)
    cpu_threads: null    # e.g. 4 or 8 for CPU; null = library default
    beam_size: 1         # 1=faster, 5=more accurate

tts:
  enabled: true
  engine: say
  voice: Daniel
